
pacman::p_load(tidyverse,
               raster, 
               rgdal,
               rgeos,
               purrr,
               raster,
               parallel,
               doParallel,
               abind,
               curl,
               zoo,
               sf,
               data.table,
               RNetCDF, 
               NicheMapR, 
               microclima,
               letsR,
               R.utils,
               rlang,
               future,
               furrr,
               future.apply, 
               futile.logger)


## ------------------------------------------------------------------------------------------------------------------------------------


## -------------------------------------------------------------------------------------------------------------------------------------------


# # Set up parallel processing
plan(multicore(workers=16))

# Set the global timeout
options(future.globals.timeout = 2700)  # Set a global timeout for 45 minutes


# Function to process each location
process_location <- function(loc) {
  
  print(paste("Processing location with lon =", loc$lon, "lat =", loc$lat))
  
  # Set parameters
  dstart <- "01/01/2005"
  dfinish <- "31/12/2015"
  
  coords<- c(loc$lon, loc$lat)
  
  # Check if current index falls within any of the problematic ranges
  ERR <- 1.5
  
  micro_result <- withTimeout({
    NicheMapR::micro_ncep(loc = coords, 
                          dstart = dstart, 
                          dfinish = dfinish, 
                          scenario=4,
                          minshade=85,
                          maxshade=90,
                          Usrhyt = 0.01,
                          cap = 1,
                          ERR = ERR, 
                          spatial = 'data/NCEP',
                          terra_source = 'data/TerraClimate/data')
  }, timeout = 600, onTimeout = "warning")
  
  if (inherits(micro_result, "try-error") || is.null(micro_result)) {
    print(paste("micro_ncep exceeded time limit for location with lon =", loc$lon, "lat =", loc$lat))
    return(list(success = FALSE, 
                loc = c(lon = loc$lon, lat = loc$lat), 
                error_message = "micro_ncep exceeded time limit"))
  } else {
    micro <- micro_result
  }
  
  
  # When the first micro_ncep fails, try again with different ERR
  if (max(micro$metout[,1] == 0)) {
    while(max(micro$metout[,1] == 0)){
      ERR <- ERR + 0.5
      
      # Use withTimeout() for the micro_ncep() function inside the while loop as well
      micro_result <- withTimeout({
        NicheMapR::micro_ncep(loc = coords, 
                              dstart = dstart, 
                              dfinish = dfinish, 
                              scenario=4,
                              minshade=85,
                              maxshade=90,
                              Usrhyt = 0.01,
                              cap = 1,
                              ERR = ERR, 
                              spatial = 'data/NCEP',
                              terra_source = 'data/TerraClimate/data')
      }, timeout = 600, onTimeout = "warning")
      
      if (inherits(micro_result, "try-error") || is.null(micro_result)) {
        print(paste("micro_ncep inside while loop exceeded time limit for location with lon =", loc$lon, "lat =", loc$lat))
        return(list(success = FALSE, 
                    loc = c(lon = loc$lon, lat = loc$lat), 
                    error_message = "micro_ncep inside while loop exceeded time limit"))
      } else {
        micro <- micro_result
      }
      
      # If ERR exceeds 5, break the loop regardless of the value of micro$metout[,1]
      if(ERR >= 5){
        break
      }
    }
  }
  
  # If even after adjusting ERR micro_ncep fails, return an error message
  if (max(micro$metout[,1] == 0)) {
    return(list(success = FALSE, 
                loc = c(lon = loc$lon, lat = loc$lat), 
                error_message = "Failed on micro_ncep call"))
  }
  
  # Another explicit check
  if (!max(micro$metout[,1] == 0)) {
    assign("micro", micro, envir = globalenv())
  } else {
    return(list(success = FALSE, 
                loc = c(lon = loc$lon, lat = loc$lat), 
                error_message = "Failed on micro_ncep call"))
  }
  
  success <- FALSE
  result <- NULL
  
  # Use withTimeout() for the ectotherm() function as well
  ecto_result <- withTimeout({
    tryCatch({
      ecto <- NicheMapR::ectotherm(live=0, 
                                   Ww_g = loc$median_mass, 
                                   shape = 4, 
                                   pct_wet = 80)
      list(success = TRUE, ecto = ecto)
    }, error = function(e) {
      list(success = FALSE,
           loc = c(lon = loc$lon, lat = loc$lat),
           error_message = paste("Failed on ectotherm call:", as.character(conditionMessage(e))))
    })
  }, timeout = 300, onTimeout = "warning")
  
  if (inherits(ecto_result, "try-error") || is.null(ecto_result)) {
    print(paste("ectotherm() exceeded time limit for location with lon =", loc$lon, "lat =", loc$lat))
    return(list(success = FALSE, 
                loc = c(lon = loc$lon, lat = loc$lat), 
                error_message = "ectotherm() exceeded time limit"))
  }
  
  if(!ecto_result$success){
    return(ecto_result)
  }
  
  gc()
  
  # Assign the successful ecto result to the global environment
  ecto <- ecto_result$ecto
  assign("ecto", ecto, envir = globalenv())
  environ <- as.data.frame(ecto$environ)
  
  # Max and mean daily temperatures
  daily_temp <- environ %>%
    dplyr::mutate(YEAR = YEAR + 2004,
                  ERR = ERR) %>%
    dplyr::group_by(ERR, YEAR, DOY, lon = paste(loc$lon), lat=paste(loc$lat)) %>%
    dplyr::summarize(max_temp = max(TC),
                     mean_temp = mean(TC), .groups = 'drop')
  
  # Create a function to calculate the rolling weekly temperature
  calc_yearly_rolling_mean <- function(data) {
    data$mean_weekly_temp <- zoo::rollapply(data$mean_temp, width = 7, FUN = mean, align = "right", partial = TRUE, fill = NA)
    data$max_weekly_temp <- zoo::rollapply(data$max_temp, width = 7, FUN = mean, align = "right", partial = TRUE, fill = NA)
    return(data)
  }
  
  # Calculate the rolling mean for each year and location
  daily_temp <- daily_temp %>%
    dplyr::group_by(YEAR, lon, lat) %>%
    dplyr::group_modify(~calc_yearly_rolling_mean(.))
  
  # Identify the warmest 91 days (3 months) of each year
  daily_temp_warmest_days <- daily_temp %>%
    dplyr::group_by(YEAR, lon, lat) %>%
    dplyr::top_n(91, max_temp)
  
  # Calculate the mean overall maximum temperature for the warmest days of each year
  overall_temp_warmest_days <- daily_temp_warmest_days %>%
    dplyr::group_by(lon, lat) %>%
    dplyr::summarize(mean = mean(max_temp),
                     median = median(max_temp),
                     fifth_percentile = quantile(max_temp, 0.05),
                     first_quartile = quantile(max_temp, 0.25),
                     third_quartile = quantile(max_temp, 0.75),
                     ninetyfifth_percentile = quantile(max_temp, 0.95),
                     min = min(max_temp),
                     max = max(max_temp), .groups = 'drop')
  
  result <- list(daily_temp, 
                 daily_temp_warmest_days, 
                 overall_temp_warmest_days)
  
  return(list(success = ecto_result$success, result = result)) # Return a list with a success flag and the result.
  
}


# Function to process a chunk of locations
process_chunk <- function(start_index, end_index) {
  # Read in distinct coordinates
  distinct_coord<- readRDS(file="RData/General_data/distinct_coordinates_adjusted.rds")
  
  distinct_coord <- distinct_coord[,3:4]
  distinct_coord <- rename(distinct_coord, 
                           x= lon,
                           y= lat)
  
  # Adjust the range of locations
  distinct_coord <- distinct_coord[start_index:end_index,]
  loc_list <- split(distinct_coord, seq(nrow(distinct_coord)))
  loc_list <- lapply(loc_list, unlist)
  
  # Match body mass data to coordinates
  presence <- readRDS(file="RData/General_data/species_coordinates_adjusted.rds")
  
  data_for_imp <- readRDS(file="RData/General_data/pre_data_for_imputation.rds")
  
  presence_body_mass <- merge(presence, dplyr::select(data_for_imp, tip.label, body_mass), by = "tip.label")
  median_body_mass <- presence_body_mass %>%
    dplyr::group_by(lon, lat) %>%
    dplyr::summarise(median_mass = median(body_mass, na.rm = TRUE)) %>%
    dplyr::ungroup()
  
  median_body_mass <- mutate(median_body_mass,
                             median_mass = ifelse(is.na(median_mass)==TRUE,
                                                  8.4,
                                                  median_mass))
  
  # Convert loc_list back into a data frame
  loc_df <- do.call("rbind", loc_list)
  loc_df <- as.data.frame(loc_df)
  names(loc_df) <- c("lon", "lat")
  
  # Join loc_df and median_body_mass
  loc_df <- dplyr::left_join(loc_df, median_body_mass, by = c("lon", "lat"))
  
  # Convert loc_df back into a list
  loc_list <- split(loc_df, seq(nrow(loc_df)))
  
  # # Set up parallel processing
  plan(multicore(workers=16))
  
  # Set the global timeout
  options(future.globals.timeout = 2700)  # Set a global timeout for 45 minutes
  
  dstart <- "01/01/2005"
  dfinish <- "31/12/2015"
  
  
  Sys.time()
  
  results <- future.apply::future_lapply(loc_list, process_location, future.packages=c("NicheMapR", 
                                                                                       "microclima", "dplyr", "zoo", "R.utils"))
  
  Sys.time()
  
  
  saveRDS(results, file=paste0("RData/Biophysical_modelling/Substrate/4C/results/12th_batch/results_biophysical_modelling_substrate_future4C_", start_index, "-", end_index, ".rds"))
  
}


# Now we can call process_chunk with different start and end indices to process the data in chunks of 16 locations.

dstart <- "01/01/2005"
dfinish <- "31/12/2015"

Sys.time()

chunk_size <- 16

# Define start and end row numbers in distinct_coord
start_row <- 11001
end_row <- 12000

# Calculate total chunks for the specified range
total_chunks <- ceiling((end_row - start_row + 1) / chunk_size)

# Loop through each chunk
for(i in seq(total_chunks)) {
  # Calculate start and end indices for the current chunk
  start_index <- ((i - 1) * chunk_size) + start_row
  end_index <- min(i * chunk_size + start_row - 1, end_row)
  
  # Call the process_chunk function with a timeout of 600 seconds
  result <- process_chunk(start_index, end_index)
}

Sys.time()
